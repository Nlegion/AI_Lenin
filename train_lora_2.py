import os
import json
import torch
import gc
import traceback
from peft import LoraConfig, get_peft_model, PeftModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from trl import SFTTrainer

# ======================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø (–ê–î–†–ï–°–ê)
# ======================
MODEL_DIR = "models/saiga"
DATASET_PATH = "data/finetune/lenin_lora_final.jsonl"
OUTPUT_DIR = "models/saiga/lora_adapter"
MERGED_MODEL_DIR = "models/saiga/merged_model"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(MERGED_MODEL_DIR, exist_ok=True)

# ======================
# –ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í
# ======================
print("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏...")
required_files = ["config.json", "model.safetensors.index.json", "tokenizer.json"]

for file in required_files:
    if not os.path.exists(os.path.join(MODEL_DIR, file)):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {os.path.join(MODEL_DIR, file)}")
        exit(1)

# ======================
# –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò (–†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´)
# ======================
try:
    print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_DIR,
        use_fast=True,
        local_files_only=True,
        padding_side="right"
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.add_special_tokens({
        "eos_token": "<|eot_id|>",
        "bos_token": "<|begin_of_text|>"
    })

    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 4-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        device_map="auto",
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        local_files_only=True,
        use_cache=False
    )
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –ù–ê–°–¢–†–û–ô–ö–ê LoRA
# ======================
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ======================
# –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
# ======================
try:
    dataset = load_dataset("json", data_files=DATASET_PATH, split="train")
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")


    def format_conversation(example):
        BOS = "<|begin_of_text|>"
        EOS = "<|eot_id|>"
        formatted = BOS

        for msg in example["conversations"]:
            role = msg["role"]
            content = msg["content"].strip()
            formatted += f"<|start_header_id|>{role}<|end_header_id|>\n\n{content}{EOS}"

        return formatted


    dataset = dataset.map(
        lambda x: {"text": format_conversation(x)},
        remove_columns=["conversations"]
    )
    print(f"–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {dataset[0]['text'][:200]}...")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø (–û–ö–û–ù–ß–ê–¢–ï–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï)
# ======================
use_bf16 = torch.cuda.is_bf16_supported()

# –û–¢–ö–õ–Æ–ß–ê–ï–ú –ì–†–ê–î–ò–ï–ù–¢–ù–´–ô –ß–ï–ö–ü–û–ò–ù–¢–ò–ù–ì
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    optim="paged_adamw_8bit",
    logging_steps=10,
    save_strategy="epoch",
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    gradient_checkpointing=False,  # –û–¢–ö–õ–Æ–ß–ï–ù–û –î–õ–Ø –†–ï–®–ï–ù–ò–Ø –ü–†–û–ë–õ–ï–ú–´
    group_by_length=True,
    bf16=use_bf16,
    fp16=not use_bf16,
    report_to="none",
    remove_unused_columns=True,
    label_names=["input_ids", "attention_mask"]
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# ======================
# –û–ë–£–ß–ï–ù–ò–ï (–° –û–¢–ö–õ–Æ–ß–ï–ù–ù–´–ú –ì–†–ê–î–ò–ï–ù–¢–ù–´–ú –ß–ï–ö–ü–û–ò–ù–¢–ò–ù–ì–û–ú)
# ======================
try:
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        dataset_text_field="text",
        max_seq_length=1024,
        packing=False,
        data_collator=data_collator,
        neftune_noise_alpha=5,
    )

    print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    model.train()
    trainer.train()
    print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô (CPU)
# ======================
try:
    del model
    del trainer
    torch.cuda.empty_cache()
    gc.collect()

    print("üß© –ù–∞—á–∞–ª–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ CPU...")

    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        torch_dtype=torch.float16,
        device_map="cpu",
        low_cpu_mem_usage=True
    )

    lora_model = PeftModel.from_pretrained(
        base_model,
        OUTPUT_DIR,
        device_map="cpu"
    )

    merged_model = lora_model.merge_and_unload()

    merged_model.save_pretrained(MERGED_MODEL_DIR)
    tokenizer.save_pretrained(MERGED_MODEL_DIR)
    print(f"üíæ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {MERGED_MODEL_DIR}")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}")
    traceback.print_exc()

print("\n–°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
print(f"1. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ GGUF: python llama.cpp/convert.py {MERGED_MODEL_DIR}")
print("2. –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ: ./llama.cpp/quantize lenin_model.f16.gguf lenin_model.Q4_K_M.gguf Q4_K_M")
print("3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å llama.cpp –¥–ª—è API")