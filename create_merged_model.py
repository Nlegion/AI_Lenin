import os
import json
import torch
import gc
import traceback
from peft import LoraConfig, get_peft_model, PeftModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from trl import SFTTrainer

# ======================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ======================
MODEL_DIR = "models/saiga"
DATASET_PATH = "data/finetune/lenin_lora_final.jsonl"
OUTPUT_DIR = "models/saiga/lora_adapter"  # –ù–æ–≤–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –∞–¥–∞–ø—Ç–µ—Ä–∞
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ======================
# –ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í –ú–û–î–ï–õ–ò
# ======================
print("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏...")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ sharded –º–æ–¥–µ–ª–∏
safetensors_files = [f for f in os.listdir(MODEL_DIR) if f.startswith('model-') and f.endswith('.safetensors')]
index_file = os.path.join(MODEL_DIR, "model.safetensors.index.json")

if not safetensors_files:
    print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ .safetensors")
    exit(1)

if not os.path.exists(index_file):
    print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω index —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏")
    exit(1)

print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(safetensors_files)} sharded —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏")
print(f"‚úÖ –ù–∞–π–¥–µ–Ω index —Ñ–∞–π–ª: {index_file}")

# ======================
# –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ò –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê
# ======================
try:
    print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_DIR,
        use_fast=True,
        local_files_only=True,
        padding_side="right"
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.add_special_tokens({
        "eos_token": "<|eot_id|>",
        "bos_token": "<|begin_of_text|>"
    })

    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 4-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        device_map="auto",
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        local_files_only=True,
        use_cache=False
    )

    print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –ù–ê–°–¢–†–û–ô–ö–ê LoRA
# ======================
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ======================
# –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
# ======================
try:
    dataset = load_dataset("json", data_files=DATASET_PATH, split="train")
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")


    def format_conversation(example):
        BOS = "<|begin_of_text|>"
        EOS = "<|eot_id|>"
        formatted = BOS

        for msg in example["conversations"]:
            role = msg["role"]
            content = msg["content"].strip()
            formatted += f"<|start_header_id|>{role}<|end_header_id|>\n\n{content}{EOS}"

        return formatted


    dataset = dataset.map(
        lambda x: {"text": format_conversation(x)},
        remove_columns=["conversations"]
    )
    print(f"–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {dataset[0]['text'][:200]}...")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø
# ======================
use_bf16 = torch.cuda.is_bf16_supported()

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-5,
    optim="paged_adamw_8bit",
    logging_steps=10,
    save_steps=50,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 50 —à–∞–≥–æ–≤
    save_total_limit=3,  # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    gradient_checkpointing=False,
    group_by_length=True,
    bf16=use_bf16,
    fp16=not use_bf16,
    report_to="none",
    remove_unused_columns=True,
    label_names=["input_ids", "attention_mask"]
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# ======================
# –û–ë–£–ß–ï–ù–ò–ï
# ======================
try:
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        dataset_text_field="text",
        max_seq_length=1024,
        packing=False,
        data_collator=data_collator,
        neftune_noise_alpha=5,
    )

    print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    model.config.use_cache = False
    trainer.train()
    print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∞–¥–∞–ø—Ç–µ—Ä–∞
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    # –°–æ–∑–¥–∞–µ–º README —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –∞–¥–∞–ø—Ç–µ—Ä–µ
    with open(os.path.join(OUTPUT_DIR, "README.md"), "w") as f:
        f.write("# –ê–¥–∞–ø—Ç–µ—Ä –õ–µ–Ω–∏–Ω–∞ –¥–ª—è Saiga3\n\n")
        f.write("## –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n")
        f.write(f"- –ú–æ–¥–µ–ª—å: {MODEL_DIR}\n")
        f.write(f"- –î–∞—Ç–∞—Å–µ—Ç: {DATASET_PATH}\n")
        f.write(f"- –≠–ø–æ—Ö–∏: 3\n")
        f.write(f"- Learning rate: 2e-5\n")
        f.write(f"- LoRA r: 16\n")
        f.write(f"- LoRA alpha: 32\n")

    print(f"üíæ –ê–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {OUTPUT_DIR}")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}")
    traceback.print_exc()
    exit(1)

print("\nüéâ –ê–¥–∞–ø—Ç–µ—Ä —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
print("\n–°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
print("1. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∞–¥–∞–ø—Ç–µ—Ä–æ–º: python merge_model.py")
print("2. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ GGUF")
print("3. –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ")