import requests
import json
import time
import subprocess
import threading


def check_gpu_usage():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU —á–µ—Ä–µ–∑ nvidia-smi"""
    try:
        result = subprocess.run([
            'nvidia-smi',
            '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu',
            '--format=csv,noheader,nounits'
        ], capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            gpu_info = result.stdout.strip().split(', ')
            if len(gpu_info) >= 4:
                utilization = gpu_info[0]
                memory_used = gpu_info[1]
                memory_total = gpu_info[2]
                temperature = gpu_info[3]
                print(f"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: {utilization}%")
                print(f"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {memory_used} MB / {memory_total} MB")
                print(f"üå°Ô∏è  –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ GPU: {temperature}¬∞C")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ GPU: {e}")


def start_llama_server():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç llama.cpp server –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ"""
    server_path = "P:/AI_Lenin/llama.cpp/llama-server.exe"
    model_path = "P:/AI_Lenin/models/saiga/gguf/saiga_llama3_8b_q4_K.gguf"

    command = [
        server_path,
        "-m", model_path,
        "--n-gpu-layers", "35",
        "--host", "127.0.0.1",
        "--port", "8080",
        "--ctx-size", "2048"
    ]

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
    process = subprocess.Popen(
        command,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        cwd="P:/AI_Lenin/llama.cpp"
    )

    # –î–∞–µ–º —Å–µ—Ä–≤–µ—Ä—É –≤—Ä–µ–º—è –Ω–∞ –∑–∞–ø—É—Å–∫
    time.sleep(5)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—Å—Ç–∏–ª—Å—è –ª–∏ —Å–µ—Ä–≤–µ—Ä
    if process.poll() is not None:
        print("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª—Å—è")
        stdout, stderr = process.communicate()
        print(f"STDOUT: {stdout.decode()}")
        print(f"STDERR: {stderr.decode()}")
        return None

    print("‚úÖ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω")
    return process


def test_with_server():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ HTTP API"""
    url = "http://127.0.0.1:8080/completion"

    # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞
    test_prompts = [
        "–û—Ç–≤–µ—Ç—å —Ç–æ—á–Ω–æ –∏ –∫—Ä–∞—Ç–∫–æ, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π: –ö–∞–∫–æ–π —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –æ–∫–µ–∞–Ω –Ω–∞ –ó–µ–º–ª–µ?",
        "–û—Ç–≤–µ—Ç—å —Ç–æ—á–Ω–æ –∏ –∫—Ä–∞—Ç–∫–æ, —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞: –°—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏?",
        "–û—Ç–≤–µ—Ç—å —Ç–æ—á–Ω–æ –∏ –∫—Ä–∞—Ç–∫–æ, —Ç–æ–ª—å–∫–æ —Ñ–∞–º–∏–ª–∏—é –∞–≤—Ç–æ—Ä–∞: –ö—Ç–æ –Ω–∞–ø–∏—Å–∞–ª '–í–æ–π–Ω—É –∏ –º–∏—Ä'?"
    ]

    for prompt in test_prompts:
        print(f"\nüß† –í–æ–ø—Ä–æ—Å: {prompt}")

        data = {
            "prompt": prompt,
            "n_predict": 20,  # –£–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –±–æ–ª–µ–µ –∫—Ä–∞—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
            "temperature": 0.1,
            "top_p": 0.9,
            "repeat_penalty": 1.2,  # –£–≤–µ–ª–∏—á–∏–º —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
            "stop": ["\n", "###"]  # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–µ –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ
        }

        start_time = time.time()
        try:
            response = requests.post(url, json=data)
            end_time = time.time()

            if response.status_code == 200:
                result = response.json()
                answer = result['content'].strip()
                print(f"üìù –û—Ç–≤–µ—Ç: {answer}")
                print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
                check_gpu_usage()
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {response.status_code}")
                print(f"–¢–µ–∫—Å—Ç –æ—à–∏–±–∫–∏: {response.text}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")

    return True


if __name__ == "__main__":
    print("üåê –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ llama.cpp server")

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä
    server_process = start_llama_server()

    if server_process is None:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä, –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É")
        exit(1)

    try:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º
        test_with_server()
    except KeyboardInterrupt:
        print("\n–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    finally:
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–µ—Ä–≤–µ—Ä
        if server_process:
            print("–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–µ—Ä–≤–µ—Ä...")
            server_process.terminate()
            server_process.wait()
            print("–°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")