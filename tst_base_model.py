from llama_cpp import Llama
import subprocess
import time


def test_optimized_gpu():
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å GPU...")

    try:
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        llm = Llama(
            model_path="models/saiga/gguf/saiga_llama3_8b_q4_K.gguf",
            n_gpu_layers=35,
            n_ctx=2048,
            n_batch=512,
            n_threads=4,
            n_threads_batch=4,
            verbose=True
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU
        print("üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU...")
        try:
            result = subprocess.run([
                'nvidia-smi',
                '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True, timeout=10)

            if result.returncode == 0:
                gpu_info = result.stdout.strip().split(', ')
                if len(gpu_info) >= 4:
                    utilization = gpu_info[0]
                    memory_used = gpu_info[1]
                    memory_total = gpu_info[2]
                    temperature = gpu_info[3]
                    print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: {utilization}%")
                    print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {memory_used} MB / {memory_total} MB")
                    print(f"‚úÖ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ GPU: {temperature}¬∞C")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ GPU: {e}")

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏
        test_questions = [
            "–û—Ç–≤–µ—Ç—å —Ç–æ—á–Ω–æ –∏ –∫—Ä–∞—Ç–∫–æ: –ö–∞–∫–æ–π —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –æ–∫–µ–∞–Ω –Ω–∞ –ó–µ–º–ª–µ?",
            "–û—Ç–≤–µ—Ç—å —Ç–æ—á–Ω–æ –∏ –∫—Ä–∞—Ç–∫–æ: –°—Ç–æ–ª–∏—Ü–∞ –†–æ—Å—Å–∏–∏?",
            "–û—Ç–≤–µ—Ç—å —Ç–æ—á–Ω–æ –∏ –∫—Ä–∞—Ç–∫–æ: –ö—Ç–æ –Ω–∞–ø–∏—Å–∞–ª '–í–æ–π–Ω—É –∏ –º–∏—Ä'?"
        ]

        for question in test_questions:
            print(f"\nüß† –í–æ–ø—Ä–æ—Å: {question}")
            start_time = time.time()
            response = llm(
                question,
                max_tokens=50,
                temperature=0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                top_p=0.9,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
                stop=["\n\n"]  # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ –¥–≤–æ–π–Ω–æ–º –ø–µ—Ä–µ–Ω–æ—Å–µ —Å—Ç—Ä–æ–∫–∏
            )
            end_time = time.time()

            answer = response['choices'][0]['text'].strip()
            print(f"üìù –û—Ç–≤–µ—Ç: {answer}")
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")

        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        return False


if __name__ == "__main__":
    test_optimized_gpu()