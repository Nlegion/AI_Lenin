import os
import json
import torch
import gc
import traceback
from peft import PeftModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
)

# ======================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ======================
MODEL_DIR = "models/saiga"
ADAPTER_DIR = "models/saiga/lora_adapter"  # –ù–æ–≤—ã–π –∞–¥–∞–ø—Ç–µ—Ä
MERGED_MODEL_DIR = "models/saiga/merged_model"  # –ù–æ–≤–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
os.makedirs(MERGED_MODEL_DIR, exist_ok=True)

# ======================
# –ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í
# ======================
print("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏...")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ sharded –º–æ–¥–µ–ª–∏
safetensors_files = [f for f in os.listdir(MODEL_DIR) if f.startswith('model-') and f.endswith('.safetensors')]
index_file = os.path.join(MODEL_DIR, "model.safetensors.index.json")

if not safetensors_files:
    print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ .safetensors")
    exit(1)

if not os.path.exists(index_file):
    print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω index —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏")
    exit(1)

print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(safetensors_files)} sharded —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏")
print(f"‚úÖ –ù–∞–π–¥–µ–Ω index —Ñ–∞–π–ª: {index_file}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞
if not os.path.exists(ADAPTER_DIR):
    print(f"‚ùå –ê–¥–∞–ø—Ç–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {ADAPTER_DIR}")
    exit(1)

adapter_files = os.listdir(ADAPTER_DIR)
print(f"–§–∞–π–ª—ã –∞–¥–∞–ø—Ç–µ—Ä–∞: {adapter_files}")

# ======================
# –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ò –ê–î–ê–ü–¢–ï–†–ê
# ======================
try:
    print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_DIR,
        use_fast=True,
        local_files_only=True,
        padding_side="right"
    )

    print("–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        device_map="auto",
        torch_dtype=torch.float16,
        local_files_only=True,
        use_cache=False
    )

    print("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    print("–ó–∞–≥—Ä—É–∑–∫–∞ –∞–¥–∞–ø—Ç–µ—Ä–∞...")
    model = PeftModel.from_pretrained(
        model,
        ADAPTER_DIR,
        device_map="auto",
        torch_dtype=torch.float16
    )

    print("‚úÖ –ê–¥–∞–ø—Ç–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
# ======================
try:
    print("üß© –ù–∞—á–∞–ª–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–æ–¥–µ–ª—å —Å –∞–¥–∞–ø—Ç–µ—Ä–æ–º
    merged_model = model.merge_and_unload()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    merged_model.save_pretrained(
        MERGED_MODEL_DIR,
        safe_serialization=True
    )
    tokenizer.save_pretrained(MERGED_MODEL_DIR)

    print(f"üíæ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {MERGED_MODEL_DIR}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open(os.path.join(MERGED_MODEL_DIR, "config.json"), "w") as f:
        json.dump(merged_model.config.to_dict(), f, indent=2)

    print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –ü–†–û–í–ï–†–ö–ê –û–ë–™–ï–î–ò–ù–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò
# ======================
try:
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    test_model = AutoModelForCausalLM.from_pretrained(
        MERGED_MODEL_DIR,
        device_map="cpu",
        torch_dtype=torch.float16,
        local_files_only=True
    )

    test_tokenizer = AutoTokenizer.from_pretrained(
        MERGED_MODEL_DIR,
        local_files_only=True
    )

    print("‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞")

    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
    del test_model
    del test_tokenizer
    gc.collect()
    torch.cuda.empty_cache()

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {str(e)}")
    traceback.print_exc()
    exit(1)

print("\nüéâ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
print("\n–°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
print(
    "1. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ GGUF: python llama.cpp/convert.py models/saiga/merged_model_new --outtype f16 --outfile models/saiga/lenin_merged_new.f16.gguf")
print(
    "2. –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ: ./llama.cpp/quantize models/saiga/lenin_merged_new.f16.gguf models/saiga/lenin_merged_new.q4_k.gguf Q4_K_M")
print("3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: python test_merged_model.py --model models/saiga/lenin_merged_new.q4_k.gguf")