import os
import json
import torch
import traceback
from peft import LoraConfig, get_peft_model
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig
)
from datasets import load_dataset
from trl import SFTTrainer

# ======================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ======================
MODEL_DIR = "models/saiga"  # –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
DATASET_PATH = "data/finetune/lenin_lora_final.jsonl"
OUTPUT_DIR = "models/saiga/lora_adapter"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ======================
# –ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í
# ======================
print("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏...")

# –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ —Ñ–∞–π–ª—ã
required_files = [
    "config.json",
    "model.safetensors.index.json",
    "tokenizer.json",
    "special_tokens_map.json"
]

for file in required_files:
    path = os.path.join(MODEL_DIR, file)
    if not os.path.exists(path):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")
        exit(1)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —à–∞—Ä–¥–æ–≤ –º–æ–¥–µ–ª–∏
index_path = os.path.join(MODEL_DIR, "model.safetensors.index.json")
with open(index_path, "r") as f:
    index_data = json.load(f)

shard_files = set(index_data["weight_map"].values())

for shard in shard_files:
    path = os.path.join(MODEL_DIR, shard)
    if not os.path.exists(path):
        print(f"‚ùå –®–∞—Ä–¥ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")
        exit(1)

print("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–∞–π–¥–µ–Ω—ã")

# ======================
# –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ò –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê
# ======================
print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_DIR,
    use_fast=True,
    local_files_only=True
)
tokenizer.pad_token = tokenizer.eos_token

# –î–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ —á–∞—Ç–∞ Llama3
tokenizer.add_special_tokens({
    "eos_token": "<|eot_id|>",
    "bos_token": "<|begin_of_text|>"
})

print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
try:
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è 4-–±–∏—Ç–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        device_map="auto",
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        local_files_only=True
    )
    print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –ù–ê–°–¢–†–û–ô–ö–ê LoRA
# ======================
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ======================
# –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
# ======================
print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
try:
    dataset = load_dataset("json", data_files=DATASET_PATH, split="train")
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")


    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –≤ —Ç–µ–∫—Å—Ç
    def format_conversation(example):
        conversation = example["conversations"]
        text = ""

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ —á–∞—Ç–∞ Llama3
        BOS = "<|begin_of_text|>"
        EOS = "<|eot_id|>"
        SYSTEM_HEADER = "<|start_header_id|>system<|end_header_id|>"
        USER_HEADER = "<|start_header_id|>user<|end_header_id|>"
        ASSISTANT_HEADER = "<|start_header_id|>assistant<|end_header_id|>"

        for message in conversation:
            role = message["role"]
            content = message["content"].strip()

            if role == "system":
                text += f"{BOS}{SYSTEM_HEADER}\n\n{content}{EOS}"
            elif role == "user":
                text += f"{USER_HEADER}\n\n{content}{EOS}"
            elif role == "assistant":
                text += f"{ASSISTANT_HEADER}\n\n{content}{EOS}"

        return {"text": text}


    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
    print("–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    dataset = dataset.map(format_conversation)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    sample_text = dataset[0]["text"]
    print(f"–ü—Ä–∏–º–µ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n{sample_text[:200]}...")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø
# ======================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=2,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=1e-4,
    optim="paged_adamw_8bit",
    fp16=True,
    logging_steps=5,
    save_strategy="epoch",
    report_to="none",
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False}
)

# ======================
# –°–û–ó–î–ê–ù–ò–ï –¢–†–ï–ù–ï–†–ê
# ======================
print("–°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞...")
try:
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        dataset_text_field="text",
        max_seq_length=256,
        packing=False
    )
    print("‚úÖ –¢—Ä–µ–Ω–µ—Ä —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –û–ë–£–ß–ï–ù–ò–ï
# ======================
print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è LoRA...")
try:
    trainer.train()
    print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {str(e)}")
    traceback.print_exc()
    exit(1)

# ======================
# –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
# ======================
try:
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
    model.save_pretrained(OUTPUT_DIR)
    print(f"üíæ LoRA –∞–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {OUTPUT_DIR}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"üíæ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {OUTPUT_DIR}")

    print(f"üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(e)}")
    traceback.print_exc()